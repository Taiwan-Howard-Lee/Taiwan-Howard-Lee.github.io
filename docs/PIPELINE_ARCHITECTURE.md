# ğŸ—ï¸ Apple ML Trading - Pipeline Architecture

## ğŸ“Š **Complete Data Pipeline Overview**

The Apple ML Trading system now features a comprehensive, professional-grade data pipeline that orchestrates the entire data flow from collection to analysis-ready features.

## ğŸ¯ **Pipeline Architecture**

### **Data Flow Diagram**
```
APIs â†’ Collection â†’ Validation â†’ Processing â†’ Features â†’ Export â†’ Models
  â†“        â†“           â†“           â†“          â†“        â†“       â†“
Logs â†’ Monitoring â†’ Quality â†’ Cleaning â†’ Indicators â†’ Storage â†’ Dashboard
```

### **Pipeline Stages**
1. **ğŸ“Š Data Collection** - Multi-source API data gathering
2. **ğŸ” Data Validation** - Quality checks and integrity verification  
3. **âš™ï¸ Data Processing** - Cleaning, transformation, and preparation
4. **ğŸ”§ Feature Engineering** - Technical indicators and derived features
5. **ğŸ“¤ Data Export** - Final dataset preparation and storage

## ğŸ—ï¸ **Directory Structure**

### **Organized Codebase**
```
apple_ml_trading/
â”œâ”€â”€ ğŸ“ src/                          # Core application code
â”‚   â”œâ”€â”€ ğŸ“ data_pipeline/            # Complete data pipeline
â”‚   â”‚   â”œâ”€â”€ ğŸ“ collectors/           # Data collection modules
â”‚   â”‚   â”‚   â”œâ”€â”€ polygon_data_collector.py
â”‚   â”‚   â”‚   â”œâ”€â”€ polygon_collector.py
â”‚   â”‚   â”‚   â”œâ”€â”€ trading_economics_collector.py
â”‚   â”‚   â”‚   â””â”€â”€ enhanced_collector.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ processors/           # Data processing & ETL
â”‚   â”‚   â”‚   â””â”€â”€ data_processor.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“ validators/           # Data quality validation
â”‚   â”‚   â”‚   â””â”€â”€ data_validator.py
â”‚   â”‚   â””â”€â”€ ğŸ“ orchestrators/        # Pipeline orchestration
â”‚   â”‚       â””â”€â”€ pipeline_orchestrator.py
â”‚   â”œâ”€â”€ ğŸ“ feature_engineering/      # Technical indicators & features
â”‚   â”œâ”€â”€ ğŸ“ models/                   # ML models & training
â”‚   â””â”€â”€ ğŸ“ utils/                    # Shared utilities
â”œâ”€â”€ ğŸ“ data/                         # Organized data storage
â”‚   â”œâ”€â”€ ğŸ“ raw/                      # Raw API data (immutable)
â”‚   â”‚   â”œâ”€â”€ ğŸ“ polygon/              # Polygon.io data
â”‚   â”‚   â”œâ”€â”€ ğŸ“ trading_economics/    # Trading Economics data
â”‚   â”‚   â””â”€â”€ ğŸ“ sessions/             # Collection sessions
â”‚   â”œâ”€â”€ ğŸ“ processed/                # Cleaned & processed data
â”‚   â”œâ”€â”€ ğŸ“ features/                 # Feature engineered data
â”‚   â”œâ”€â”€ ğŸ“ models/                   # Trained model artifacts
â”‚   â”œâ”€â”€ ğŸ“ exports/                  # Final datasets
â”‚   â”œâ”€â”€ ğŸ“ validation_reports/       # Data quality reports
â”‚   â””â”€â”€ ğŸ“ processing_reports/       # Processing summaries
â”œâ”€â”€ ğŸ“ config/                       # Configuration management
â”‚   â”œâ”€â”€ ğŸ“ pipelines/                # Pipeline configurations
â”‚   â”‚   â””â”€â”€ default_pipeline.json
â”‚   â”œâ”€â”€ ğŸ“ environments/             # Environment-specific configs
â”‚   â””â”€â”€ ğŸ“ models/                   # Model configurations
â”œâ”€â”€ ğŸ“ scripts/                      # Automation & utility scripts
â”‚   â”œâ”€â”€ ğŸ“ data_collection/          # Collection automation
â”‚   â”‚   â”œâ”€â”€ run_continuous_collection.py
â”‚   â”‚   â”œâ”€â”€ run_enhanced_collection.py
â”‚   â”‚   â”œâ”€â”€ compare_collection_strategies.py
â”‚   â”‚   â””â”€â”€ test_*.py
â”‚   â”œâ”€â”€ ğŸ“ pipeline/                 # Pipeline management
â”‚   â”‚   â””â”€â”€ run_pipeline.py
â”‚   â””â”€â”€ ğŸ“ deployment/               # Deployment scripts
â”œâ”€â”€ ğŸ“ tests/                        # Comprehensive test suite
â”‚   â”œâ”€â”€ ğŸ“ unit/                     # Unit tests
â”‚   â”œâ”€â”€ ğŸ“ integration/              # Integration tests
â”‚   â””â”€â”€ ğŸ“ data/                     # Data quality tests
â”œâ”€â”€ ğŸ“ docs/                         # Documentation
â”‚   â”œâ”€â”€ ğŸ“ user_guides/              # User guides
â”‚   â”‚   â”œâ”€â”€ README_backup.md
â”‚   â”‚   â”œâ”€â”€ QUICK_START.md
â”‚   â”‚   â”œâ”€â”€ DATA_DIVERSITY_STRATEGY.md
â”‚   â”‚   â””â”€â”€ CODEBASE_REORGANIZATION_PLAN.md
â”‚   â”œâ”€â”€ ğŸ“ api/                      # API documentation
â”‚   â””â”€â”€ ğŸ“ pipeline/                 # Pipeline documentation
â”œâ”€â”€ ğŸ“ logs/                         # Application logs
â””â”€â”€ ğŸ“ notebooks/                    # Jupyter notebooks
```

## ğŸš€ **Pipeline Components**

### **1. Data Collection Layer**
- **PolygonDataCollector**: Professional US market data collection
- **TradingEconomicsCollector**: Global economic indicators
- **EnhancedCollector**: Maximum diversity collection strategy
- **Rate limiting**: Intelligent API usage optimization
- **Error handling**: Robust retry mechanisms

### **2. Data Validation Layer**
- **Comprehensive validation**: Price, volume, date consistency
- **Quality metrics**: OHLC logic, missing data detection
- **Automated reporting**: JSON validation reports
- **Configurable thresholds**: Customizable quality standards

### **3. Data Processing Layer**
- **Data cleaning**: Duplicate removal, outlier detection
- **Transformation**: Returns calculation, normalization
- **Missing data handling**: Interpolation, forward fill
- **Quality assurance**: Automated data quality checks

### **4. Feature Engineering Layer**
- **Technical indicators**: RSI, MACD, Bollinger Bands, Stochastic
- **Economic features**: Integration with Trading Economics data
- **Derived features**: Returns, volatility, momentum indicators
- **Configurable parameters**: Customizable indicator settings

### **5. Orchestration Layer**
- **Pipeline orchestrator**: Complete workflow management
- **Stage coordination**: Sequential and parallel execution
- **Error recovery**: Intelligent failure handling
- **Progress monitoring**: Real-time pipeline status

## âš™ï¸ **Configuration Management**

### **Pipeline Configuration (default_pipeline.json)**
```json
{
  "pipeline": {
    "name": "apple_ml_trading_pipeline",
    "version": "2.0.0",
    "stages": [
      "data_collection",
      "data_validation", 
      "data_processing",
      "feature_engineering",
      "data_export"
    ]
  },
  "data_collection": {
    "sources": ["polygon", "trading_economics"],
    "rate_limiting": {
      "polygon": {"requests_per_minute": 5},
      "trading_economics": {"requests_per_minute": 60}
    }
  },
  "data_validation": {
    "price_validation": {"min_price": 1.0, "max_price": 1000.0},
    "quality_thresholds": {"min_pass_rate": 0.90}
  },
  "feature_engineering": {
    "technical_indicators": {
      "momentum": {"rsi": {"period": 14}, "macd": {"fast": 12, "slow": 26}},
      "trend": {"sma": {"periods": [5, 10, 20, 50, 200]}}
    }
  }
}
```

## ğŸš€ **Usage Commands**

### **Complete Pipeline Execution**
```bash
# Run full pipeline
python3 scripts/pipeline/run_pipeline.py

# Dry run (show what would be executed)
python3 scripts/pipeline/run_pipeline.py --dry-run

# Run specific stages
python3 scripts/pipeline/run_pipeline.py --stages data_collection data_validation

# Check pipeline status
python3 scripts/pipeline/run_pipeline.py status
```

### **Individual Component Testing**
```bash
# Test data collection
python3 src/data_pipeline/collectors/polygon_data_collector.py

# Test data validation
python3 src/data_pipeline/validators/data_validator.py

# Test data processing
python3 src/data_pipeline/processors/data_processor.py

# Test pipeline orchestrator
python3 src/data_pipeline/orchestrators/pipeline_orchestrator.py
```

### **Enhanced Data Collection**
```bash
# Maximum diversity collection
python3 scripts/data_collection/run_enhanced_collection.py --hours 8

# Compare collection strategies
python3 scripts/data_collection/compare_collection_strategies.py
```

## ğŸ“Š **Data Quality & Monitoring**

### **Validation Reports**
- **Location**: `data/validation_reports/`
- **Format**: JSON with detailed quality metrics
- **Frequency**: Generated after each validation stage
- **Content**: File-level and aggregate quality scores

### **Processing Reports**
- **Location**: `data/processing_reports/`
- **Format**: JSON with processing statistics
- **Metrics**: Records processed, cleaning operations, duration
- **Quality tracking**: Before/after data quality comparison

### **Pipeline Logs**
- **Location**: `logs/pipeline.log`
- **Level**: Configurable (DEBUG, INFO, WARNING, ERROR)
- **Rotation**: Automatic log rotation with size limits
- **Monitoring**: Real-time pipeline status tracking

## ğŸ¯ **Benefits Achieved**

### **Code Organization**
- âœ… **Professional structure** - Clear separation of concerns
- âœ… **Maintainable codebase** - Easy navigation and updates
- âœ… **Scalable architecture** - Ready for additional features
- âœ… **Comprehensive testing** - Unit and integration test support

### **Data Pipeline**
- âœ… **Automated workflow** - End-to-end data processing
- âœ… **Quality assurance** - Comprehensive validation and monitoring
- âœ… **Error resilience** - Robust error handling and recovery
- âœ… **Performance optimization** - Efficient data processing

### **Developer Experience**
- âœ… **Easy deployment** - Simple command-line interface
- âœ… **Clear documentation** - Comprehensive guides and examples
- âœ… **Flexible configuration** - Customizable pipeline behavior
- âœ… **Monitoring tools** - Real-time status and quality tracking

## ğŸš€ **Next Steps**

1. **Run the complete pipeline**: `python3 scripts/pipeline/run_pipeline.py`
2. **Monitor data quality**: Check validation reports in `data/validation_reports/`
3. **Customize configuration**: Edit `config/pipelines/default_pipeline.json`
4. **Add new features**: Extend collectors, processors, or validators
5. **Scale the system**: Add parallel processing and cloud deployment

**ğŸ‰ Your Apple ML Trading system now has a professional-grade, production-ready data pipeline!**
